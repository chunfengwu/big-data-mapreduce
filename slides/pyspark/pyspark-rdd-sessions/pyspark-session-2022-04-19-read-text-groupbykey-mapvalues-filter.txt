% cat /tmp/movies.txt
user9,m1,5
user8,m2,4
user1,m1,2
user1,9
user1,m1,2
user2,m2,3
user2,m3,5
user3,m3,4
user6,m3,4
user7,m3,3
user3,king
user4,m1,3
user5,m2,5
user6,m4,5
user7,m5,5
user1
user3,m3,5
user4,m4,1

% ./bin/pyspark
Python 3.8.9 (default, Mar 30 2022, 13:51:17)
[Clang 13.1.6 (clang-1316.0.21.2.3)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/

Using Python version 3.8.9 (default, Mar 30 2022 13:51:17)
Spark context Web UI available at http://10.0.0.234:4041
Spark context available as 'sc' (master = local[*], app id = local-1650425312842).
SparkSession available as 'spark'.
>>>
>>>
>>>
>>> input_path = "/tmp/movies.txt"
>>> input_path
'/tmp/movies.txt'
>>> # read input and create RDD[String]
>>> records = spark.sparkContext.textFile(input_path)
>>> records.collect()
[
  'user9,m1,5', 
  'user8,m2,4', 
  'user1,m1,2', 
  'user1,9', 
  'user1,m1,2', 
  'user2,m2,3', 
  'user2,m3,5', 
  'user3,m3,4', 
  'user6,m3,4', 
  'user7,m3,3', 
  'user3,king', 
  'user4,m1,3', 
  'user5,m2,5', 
  'user6,m4,5', 
  'user7,m5,5', 
  'user1', 
  'user3,m3,5', 
  'user4,m4,1'
]
>>> records.count()
18
>>>
>>>
>>> records.getNumPartitions()
2
>>>
>>>
>>>
>>> pairs = [("A", 3), ("A", 4), ("A", 5), ("B", 30), ("B", 40), ("B", 50), ("B", 60), ("C", 100)]
>>> pairs
[('A', 3), ('A', 4), ('A', 5), ('B', 30), ('B', 40), ('B', 50), ('B', 60), ('C', 100)]
>>> rdd = spark.sparkContext.parallelize(pairs)
>>> rdd.collect()
[('A', 3), ('A', 4), ('A', 5), ('B', 30), ('B', 40), ('B', 50), ('B', 60), ('C', 100)]
>>> rdd.count()
8
>>> rdd.getNumPartitions()
16
>>> # NOTE: since the number of partitions is more than 
>>> #       the number of elements: this implies that 
>>> #       some of the partitions can be empty (partition
>>> #       is created, but has no elements at all).
>>>
>>>
# find average of values per key: A, B, C
>>> # use groupByKey() transformation
>>> grouped = rdd.groupByKey()
>>> grouped.collect()
[
 ('B', <pyspark.resultiterable.ResultIterable object at 0x1093f7b80>), 
 ('C', <pyspark.resultiterable.ResultIterable object at 0x1094030d0>), 
 ('A', <pyspark.resultiterable.ResultIterable object at 0x109403130>)
]

>>> grouped.mapValues(lambda values: list(values)).collect()
[
 ('B', [30, 40, 50, 60]), 
 ('C', [100]), 
 ('A', [3, 4, 5])
]
>>> # similar to SQL's GROUP BY
>>> # values : ResultIterable
>>> avg_by_key = grouped.mapValues(lambda values: sum(values) / len(values))
>>> avg_by_key.collect()
[('B', 45.0), ('C', 100.0), ('A', 4.0)]
>>>
>>>
>>> rdd.collect()
[('A', 3), ('A', 4), ('A', 5), ('B', 30), ('B', 40), ('B', 50), ('B', 60), ('C', 100)]
>>> rdd_44 = rdd.mapValues(lambda v : v * 10)
>>> rdd_44.collect()
[('A', 30), ('A', 40), ('A', 50), ('B', 300), ('B', 400), ('B', 500), ('B', 600), ('C', 1000)]
>>> # v : denotes the value component of (key, value) pair.
>>>
>>>
>>> # apply a filter and keep (key, value) pairs 
>>> # if and only if value id greter than 100
>>>
>>> # understand tuple of 2 elements as (key, value) pair:
>>> x = ("K", 2345)
>>> x[0]
'K'
>>> x[1]
2345
>>>
>>>
>>> # apply a filter to rdd_44 and keep (key, value) 
>>> # pairs if and only if value is greter than 100
>>> # x denotes a single element of source RDD (rdd_44)
>>> rdd5 = rdd_44.filter(lambda x: x[1] > 100)
>>> rdd5.collect()
[('B', 300), ('B', 400), ('B', 500), ('B', 600), ('C', 1000)]
>>>
>>>
>>> some_lists = [ [1, 2, 3], [7, 8, 9, 10], [], [] ]
>>> len(some_lists)
4
>>> some_lists[0]
[1, 2, 3]
>>> some_lists[1]
[7, 8, 9, 10]
>>> some_lists[2]
[]
>>> some_lists[3]
[]
>>> rdd = spark.sparkContext.parallelize(some_lists)
>>> rdd.collect()
[[1, 2, 3], [7, 8, 9, 10], [], []]
>>> rdd.count()
4
>>> # each rdd element is a list denoted by [...]
>>>
>>> rdd2 = rdd.flatMap(lambda x: x)
>>> rdd2.collect()
[1, 2, 3, 7, 8, 9, 10]
>>> rdd2.count()
7
>>> rdd3 = rdd.map(lambda x: x)
>>> rdd3.collect()
[[1, 2, 3], [7, 8, 9, 10], [], []]
>>> rdd3.collect()
[[1, 2, 3], [7, 8, 9, 10], [], []]
>>> rdd3.count()
4
>>> 