$ cat  /tmp/movies.txt
user1,m1,1
user1,m1,3
user1,m1,5
user1,m1,5
user2,m2,3
user2,m3,5
user8,m3,5
user3,m3,4
user6,m3,4
user7,m3,5
user4,m1,3
user5,m2,4
user6,m4,5
user8,m4,5
user7,m4,5
user8,m5,1
user8,m5,0
user1,m1,1
user6,m3,4
user7,m3,5


$ ./bin/pyspark
Python 3.7.2 (v3.7.2:9a3ffc0492, Dec 24 2018, 02:44:43)
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Python version 3.7.2 (v3.7.2:9a3ffc0492, Dec 24 2018 02:44:43)
Spark context Web UI available at http://10.0.0.93:4041
Spark context available as 'sc' (master = local[*], app id = local-1619742411221).
SparkSession available as 'spark'.

>>> input_path = "/tmp/movies.txt"
>>> input_path
'/tmp/movies.txt'
>>> records = spark.sparkContext.textFile(input_path)
>>> records.count()
20
>>> records.collect()
['user1,m1,1', 'user1,m1,3', 'user1,m1,5', 'user1,m1,5', 'user2,m2,3', 'user2,m3,5', 'user8,m3,5', 'user3,m3,4', 'user6,m3,4', 'user7,m3,5', 'user4,m1,3', 'user5,m2,4', 'user6,m4,5', 'user8,m4,5', 'user7,m4,5', 'user8,m5,1', 'user8,m5,0', 'user1,m1,1', 'user6,m3,4', 'user7,m3,5']

>>># Create (movieid, rating) pairs
>>> def tokenize(record):
...   tokens = record.split(",")
...   movieid = tokens[0]
...   movieid = tokens[1]
...   rating = int(tokens[2])
...   return (movieid, rating)
...
>>># Test tokenize() function
>>> result = tokenize('user2,m2,3')
>>> result
('m2', 3)
>>>
>>>
>>> pairs = records.map(tokenize)
>>> pairs.collect()
[('m1', 1), ('m1', 3), ('m1', 5), ('m1', 5), ('m2', 3), ('m3', 5), ('m3', 5), ('m3', 4), ('m3', 4), ('m3', 5), ('m1', 3), ('m2', 4), ('m4', 5), ('m4', 5), ('m4', 5), ('m5', 1), ('m5', 0), ('m1', 1), ('m3', 4), ('m3', 5)]
>>>
>>># Drop records if movie rating is less than 2
>>> filtered = pairs.filter(lambda x : x[1] >=2)
>>>
>>> filtered.collect()
[('m1', 3), ('m1', 5), ('m1', 5), ('m2', 3), ('m3', 5), ('m3', 5), ('m3', 4), ('m3', 4), ('m3', 5), ('m1', 3), ('m2', 4), ('m4', 5), ('m4', 5), ('m4', 5), ('m3', 4), ('m3', 5)]

>>># Create a flatten function
>>> def flatten(keyvalue):
...   movieid = keyvalue[0]
...   rating = keyvalue[1]
...   if rating == 5:
...     return [(movieid, rating), ("unique-5", movieid)]
...   else:
...     return [(movieid, rating)]
...
>>># Flatten (k, v) pairs:
>>> flattened = filtered.flatMap(flatten)
>>>
>>> flattened.collect()
[('m1', 3), ('m1', 5), ('unique-5', 'm1'), ('m1', 5), ('unique-5', 'm1'), ('m2', 3), ('m3', 5), ('unique-5', 'm3'), ('m3', 5), ('unique-5', 'm3'), ('m3', 4), ('m3', 4), ('m3', 5), ('unique-5', 'm3'), ('m1', 3), ('m2', 4), ('m4', 5), ('unique-5', 'm4'), ('m4', 5), ('unique-5', 'm4'), ('m4', 5), ('unique-5', 'm4'), ('m3', 4), ('m3', 5), ('unique-5', 'm3')]
>>>
>>># Debug result
>>> grouped = flattened.groupByKey()
>>> grouped.mapValues(lambda values: list(values)).collect()
[('m3', [5, 5, 4, 4, 5, 4, 5]), ('m4', [5, 5, 5]), ('m1', [3, 5, 5, 3]), ('unique-5', ['m1', 'm1', 'm3', 'm3', 'm3', 'm4', 'm4', 'm4', 'm3']), ('m2', [3, 4])]
>>>
>>># Create a reduction function 
>>> def reduction(keyvalue):
...   key = keyvalue[0]
...   values = keyvalue[1]
...   if key == 'unique-5':
...     return (key, set(values))
...   else:
...     avg = sum(values) / len(values)
...     if (avg >= 3):
...       return(key, avg)
...
>>>
>>># Apply the reduction function
>>> result = grouped.map(reduction)
>>>
>>> result.collect()
[('m3', 4.571428571428571), ('m4', 5.0), ('m1', 4.0), ('unique-5', {'m4', 'm1', 'm3'}), ('m2', 3.5)]
>>>