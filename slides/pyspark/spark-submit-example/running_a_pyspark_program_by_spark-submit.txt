$ cat /tmp/books.txt
ISBN-100,sales,biology
IS-01235,sales,econ
ISBN-101,sales,econ
ISBN-102,sales,biology
ISBN-109,econ,sales
ISBN-103,CS,sales
ISBN-104,CS,biology
ISBN-105,CS,econ
ISBN-200,CS

$ wc -l /tmp/books.txt
       9 books.txt

$ cat /tmp/pyspark_0001.py
from __future__ import print_function

from pyspark.sql import SparkSession

import sys

# A SparkSession can be used create DataFrame, 
# register DataFrame as tables, execute SQL 
# over tables, cache tables, and read parquet files. 
# To create a SparkSession, use the following 
# builder pattern:

spark = SparkSession.builder\
   .appName("testing...") \
   .getOrCreate()

input_path = sys.argv[1]
print("input_path=", input_path)

records = spark.sparkContext.textFile(input_path)

print("records.count()=", records.count())

print("records.collect()=", records.collect())

spark.stop()

$ /Users/mparsian/spark-2.4.4/bin/spark-submit  /tmp/pyspark_0001.py  /tmp/books.txt
input_path= /tmp/books.txt
records.count()= 9
records.collect()= 
[
 'ISBN-100,sales,biology', 
 'IS-01235,sales,econ', 
 'ISBN-101,sales,econ', 
 'ISBN-102,sales,biology', 
 'ISBN-109,econ,sales', 
 'ISBN-103,CS,sales', 
 'ISBN-104,CS,biology', 
 'ISBN-105,CS,econ', 
 'ISBN-200,CS'
]
